Gradient Boosting:
  default:
    learning_rate: 0.1
    max_iter: 200
    max_depth: 5
    class_weight:
      0: 1
      1: 5

  grid_search:
    learning_rate: [0.01, 0.05, 0.1]
    max_iter: [100, 200]
    max_depth: [null, 5, 10]
    #class_weight:
    #  - null
    #  - {0: 1, 1: 2}

Logistic Regression:
  default:
    C: 0.1
    penalty: l2
    class_weight: null
    max_iter: 2000

  grid_search:
    max_iter: [2000]
    C: [0.01, 0.1, 1]
    penalty: [l2]
    class_weight:
      - null
      - {0: 1, 1: 2}
      - {0: 1, 1: 5}

Random Forest:
  default:
    n_estimators: 150
    max_depth: 10
    min_samples_split: 5
    class_weight:
      0: 1
      1: 1

  grid_search:
    n_estimators: [100, 200,250]
    max_depth: [5, 10]
    min_samples_split: [5,10,15]
    class_weight:
      - {0: 1, 1: 1}
      - {0: 1, 1: 2}

Xgboost:
  default:
    learning_rate: 0.1
    max_depth: 5
    n_estimators: 200
    scale_pos_weight: 2
    objective: binary:logistic
    eval_metric: logloss

  grid_search:
    learning_rate: [0.01, 0.1]
    max_depth: [3, 5]
    n_estimators: [100, 150, 200]
    scale_pos_weight: [1, 2]
    objective: [binary:logistic]
    #eval_metric: logloss



CatBoost:
  default:
    learning_rate: 0.1
    depth: 6
    n_estimators: 300

  grid_search:
    learning_rate: [0.03, 0.1]
    depth: [4, 6, 8]
    n_estimators: [200, 300, 500]

